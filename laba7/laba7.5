import requests
from bs4 import BeautifulSoup
import os

def download_images_from_url(url, visited_urls):
    # Проверяем, была ли уже посещена данная страница
    if url in visited_urls:
        return
    
    visited_urls.add(url)  # Добавляем URL в список посещенных

    try:
        # Отправляем GET-запрос
        response = requests.get(url)

        # Проверяем статус-код ответа
        if response.status_code == 200:
            print(f"Страница загружена: {url}")
            
            # Парсим HTML-страницу
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Создаем папку для сохранения изображений
            os.makedirs('images', exist_ok=True)

            # Находим все теги <img> на странице
            images = soup.find_all('img')
            
            for img in images:
                img_url = img.get('src')
                if img_url:
                    # Полный URL изображения
                    if not img_url.startswith('http'):
                        img_url = url + img_url  # Если ссылка относительная
                    
                    # Загружаем изображение
                    img_response = requests.get(img_url)
                    
                    if img_response.status_code == 200:
                        # Получаем имя файла из URL
                        img_name = os.path.join('images', img_url.split('/')[-1])
                        
                        # Сохраняем изображение
                        with open(img_name, 'wb') as f:
                            f.write(img_response.content)
                        print(f"Сохранено: {img_name}")
                    else:
                        print(f"Ошибка при загрузке изображения: {img_url}")

            # Находим все ссылки на странице
            links = soup.find_all('a')
            for link in links:
                link_url = link.get('href')
                if link_url and not link_url.startswith('#'):
                    # Полный URL ссылки
                    if not link_url.startswith('http'):
                        link_url = url + link_url  # Если ссылка относительная
                    
                    # Рекурсивно загружаем изображения с новой страницы
                    download_images_from_url(link_url, visited_urls)

        else:
            print(f"Ошибка при загрузке страницы. Статус-код: {response.status_code}")

    except requests.exceptions.RequestException as e:
        print(f"Произошла ошибка: {e}")

# Начальный URL для загрузки изображений
start_url = "http://rokot.ibst.psu/anatoly/"
visited_urls = set()  # Множество для отслеживания посещенных URL

download_images_from_url(start_url, visited_urls)
